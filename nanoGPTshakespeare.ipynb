{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/toche7/GenAIEngWeek2/blob/main/nanoGPTshakespeare.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zvPA9fIty5c",
        "outputId": "0f601ae8-59d0-4dc4-c6fe-97378eb7169f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 682, done.\u001b[K\n",
            "remote: Total 682 (delta 0), reused 0 (delta 0), pack-reused 682 (from 1)\u001b[K\n",
            "Receiving objects: 100% (682/682), 952.47 KiB | 1.38 MiB/s, done.\n",
            "Resolving deltas: 100% (385/385), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tiktoken transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cs9c2P1fvRn",
        "outputId": "b59ec3cd-f33f-4488-dd5a-94ae5f3b7621"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./nanoGPT/data/shakespeare/ && python prepare.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxoMQ8IMt8Bu",
        "outputId": "a7290346-f008-4fca-aa56-c6360c0a198e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train has 301,966 tokens\n",
            "val has 36,059 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./nanoGPT/ && python train.py --dtype=float16 --dataset=shakespeare --compile=False --n_layer=4 --n_head=4 --n_embd=64 --block_size=64 --batch_size=8 --init_from=gpt2 --eval_interval=100 --eval_iters=100 --max_iters=100 --bias=True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBXoH_7tunr6",
        "outputId": "7068805a-029e-4af0-eb3a-1fa48a6dd546"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: dtype = float16\n",
            "Overriding: dataset = shakespeare\n",
            "Overriding: compile = False\n",
            "Overriding: n_layer = 4\n",
            "Overriding: n_head = 4\n",
            "Overriding: n_embd = 64\n",
            "Overriding: block_size = 64\n",
            "Overriding: batch_size = 8\n",
            "Overriding: init_from = gpt2\n",
            "Overriding: eval_interval = 100\n",
            "Overriding: eval_iters = 100\n",
            "Overriding: max_iters = 100\n",
            "Overriding: bias = True\n",
            "tokens per iteration will be: 20,480\n",
            "Initializing from OpenAI GPT-2 weights: gpt2\n",
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 123.65M\n",
            "config.json: 100% 665/665 [00:00<00:00, 4.51MB/s]\n",
            "model.safetensors: 100% 548M/548M [00:05<00:00, 98.6MB/s]\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 680kB/s]\n",
            "/content/nanoGPT/train.py:196: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
            "num decayed parameter tensors: 50, with 123,581,184 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "step 0: train loss 4.9566, val loss 4.8729\n",
            "iter 0: loss 5.3318, time 6337.17ms, mfu -100.00%\n",
            "iter 1: loss 5.0280, time 1645.88ms, mfu -100.00%\n",
            "iter 2: loss 4.9225, time 1733.97ms, mfu -100.00%\n",
            "iter 3: loss 4.9568, time 1738.66ms, mfu -100.00%\n",
            "iter 4: loss 4.8434, time 1743.99ms, mfu -100.00%\n",
            "iter 5: loss 5.1189, time 1747.40ms, mfu 2.81%\n",
            "iter 6: loss 4.9655, time 1744.28ms, mfu 2.81%\n",
            "iter 7: loss 5.1250, time 1748.51ms, mfu 2.81%\n",
            "iter 8: loss 4.7988, time 1749.93ms, mfu 2.81%\n",
            "iter 9: loss 4.9943, time 1755.60ms, mfu 2.81%\n",
            "iter 10: loss 4.7791, time 1758.50ms, mfu 2.81%\n",
            "iter 11: loss 4.7187, time 1761.68ms, mfu 2.81%\n",
            "iter 12: loss 5.0377, time 1766.64ms, mfu 2.81%\n",
            "iter 13: loss 4.8571, time 1761.32ms, mfu 2.80%\n",
            "iter 14: loss 4.5954, time 1770.26ms, mfu 2.80%\n",
            "iter 15: loss 4.6565, time 1774.89ms, mfu 2.80%\n",
            "iter 16: loss 4.2498, time 1774.62ms, mfu 2.80%\n",
            "iter 17: loss 4.3873, time 1779.36ms, mfu 2.79%\n",
            "iter 18: loss 4.3845, time 1782.50ms, mfu 2.79%\n",
            "iter 19: loss 4.5940, time 1786.26ms, mfu 2.79%\n",
            "iter 20: loss 4.3874, time 1786.62ms, mfu 2.78%\n",
            "iter 21: loss 4.4618, time 1787.11ms, mfu 2.78%\n",
            "iter 22: loss 4.2486, time 1795.85ms, mfu 2.77%\n",
            "iter 23: loss 4.3589, time 1794.70ms, mfu 2.77%\n",
            "iter 24: loss 4.2749, time 1798.89ms, mfu 2.77%\n",
            "iter 25: loss 4.1162, time 1802.64ms, mfu 2.76%\n",
            "iter 26: loss 4.2234, time 1805.71ms, mfu 2.76%\n",
            "iter 27: loss 4.2314, time 1809.28ms, mfu 2.76%\n",
            "iter 28: loss 4.1063, time 1812.78ms, mfu 2.75%\n",
            "iter 29: loss 4.1218, time 1816.84ms, mfu 2.75%\n",
            "iter 30: loss 3.8814, time 1815.91ms, mfu 2.74%\n",
            "iter 31: loss 4.2005, time 1821.23ms, mfu 2.74%\n",
            "iter 32: loss 4.3328, time 1832.19ms, mfu 2.73%\n",
            "iter 33: loss 4.1711, time 1827.62ms, mfu 2.73%\n",
            "iter 34: loss 3.9211, time 1835.26ms, mfu 2.72%\n",
            "iter 35: loss 4.1719, time 1838.17ms, mfu 2.72%\n",
            "iter 36: loss 4.1294, time 1844.38ms, mfu 2.71%\n",
            "iter 37: loss 4.1344, time 1840.02ms, mfu 2.71%\n",
            "iter 38: loss 3.7761, time 1853.23ms, mfu 2.70%\n",
            "iter 39: loss 4.2145, time 1855.02ms, mfu 2.70%\n",
            "iter 40: loss 3.7535, time 1855.11ms, mfu 2.69%\n",
            "iter 41: loss 3.9926, time 1862.58ms, mfu 2.69%\n",
            "iter 42: loss 4.2362, time 1861.68ms, mfu 2.68%\n",
            "iter 43: loss 3.8178, time 1864.40ms, mfu 2.68%\n",
            "iter 44: loss 3.7957, time 1867.43ms, mfu 2.67%\n",
            "iter 45: loss 3.7892, time 1878.05ms, mfu 2.67%\n",
            "iter 46: loss 3.8361, time 1873.41ms, mfu 2.66%\n",
            "iter 47: loss 4.0673, time 1876.77ms, mfu 2.66%\n",
            "iter 48: loss 3.8177, time 1883.26ms, mfu 2.65%\n",
            "iter 49: loss 3.9364, time 1884.78ms, mfu 2.65%\n",
            "iter 50: loss 4.0701, time 1886.65ms, mfu 2.65%\n",
            "iter 51: loss 4.0643, time 1892.64ms, mfu 2.64%\n",
            "iter 52: loss 3.9785, time 1898.11ms, mfu 2.64%\n",
            "iter 53: loss 3.6448, time 1907.66ms, mfu 2.63%\n",
            "iter 54: loss 3.8670, time 1910.06ms, mfu 2.62%\n",
            "iter 55: loss 3.4436, time 1913.48ms, mfu 2.62%\n",
            "iter 56: loss 3.6919, time 1919.00ms, mfu 2.61%\n",
            "iter 57: loss 3.6596, time 1925.60ms, mfu 2.61%\n",
            "iter 58: loss 4.2103, time 1931.46ms, mfu 2.60%\n",
            "iter 59: loss 3.6572, time 1928.42ms, mfu 2.60%\n",
            "iter 60: loss 3.9377, time 1929.30ms, mfu 2.59%\n",
            "iter 61: loss 3.6283, time 1927.85ms, mfu 2.59%\n",
            "iter 62: loss 4.1860, time 1936.60ms, mfu 2.58%\n",
            "iter 63: loss 3.7598, time 1941.07ms, mfu 2.58%\n",
            "iter 64: loss 3.8611, time 1933.10ms, mfu 2.57%\n",
            "iter 65: loss 3.4028, time 1941.94ms, mfu 2.57%\n",
            "iter 66: loss 3.9755, time 1923.87ms, mfu 2.57%\n",
            "iter 67: loss 3.6003, time 1932.71ms, mfu 2.57%\n",
            "iter 68: loss 3.6968, time 1916.78ms, mfu 2.57%\n",
            "iter 69: loss 3.9861, time 1927.29ms, mfu 2.56%\n",
            "iter 70: loss 3.4245, time 1919.95ms, mfu 2.56%\n",
            "iter 71: loss 3.7421, time 1910.13ms, mfu 2.56%\n",
            "iter 72: loss 3.7175, time 1915.30ms, mfu 2.57%\n",
            "iter 73: loss 3.6269, time 1908.07ms, mfu 2.57%\n",
            "iter 74: loss 3.7302, time 1902.69ms, mfu 2.57%\n",
            "iter 75: loss 3.3777, time 1896.12ms, mfu 2.57%\n",
            "iter 76: loss 3.9287, time 1896.58ms, mfu 2.57%\n",
            "iter 77: loss 4.0425, time 1895.50ms, mfu 2.57%\n",
            "iter 78: loss 3.0655, time 1901.96ms, mfu 2.58%\n",
            "iter 79: loss 3.5459, time 1893.48ms, mfu 2.58%\n",
            "iter 80: loss 3.3672, time 1896.78ms, mfu 2.58%\n",
            "iter 81: loss 3.8197, time 1893.72ms, mfu 2.58%\n",
            "iter 82: loss 3.4593, time 1883.00ms, mfu 2.58%\n",
            "iter 83: loss 3.6139, time 1890.42ms, mfu 2.59%\n",
            "iter 84: loss 3.7131, time 1897.00ms, mfu 2.59%\n",
            "iter 85: loss 4.2393, time 1877.32ms, mfu 2.59%\n",
            "iter 86: loss 3.4735, time 1886.46ms, mfu 2.59%\n",
            "iter 87: loss 3.6212, time 1886.65ms, mfu 2.59%\n",
            "iter 88: loss 3.3305, time 1891.58ms, mfu 2.59%\n",
            "iter 89: loss 3.9243, time 1884.33ms, mfu 2.59%\n",
            "iter 90: loss 3.4602, time 1889.18ms, mfu 2.60%\n",
            "iter 91: loss 3.6825, time 1888.67ms, mfu 2.60%\n",
            "iter 92: loss 3.3271, time 1882.64ms, mfu 2.60%\n",
            "iter 93: loss 3.5286, time 1883.31ms, mfu 2.60%\n",
            "iter 94: loss 3.5055, time 1886.20ms, mfu 2.60%\n",
            "iter 95: loss 3.5791, time 1889.95ms, mfu 2.60%\n",
            "iter 96: loss 3.5564, time 1895.98ms, mfu 2.60%\n",
            "iter 97: loss 3.3578, time 1886.75ms, mfu 2.60%\n",
            "iter 98: loss 3.4841, time 1887.49ms, mfu 2.60%\n",
            "iter 99: loss 3.5615, time 1890.12ms, mfu 2.60%\n",
            "step 100: train loss 3.5421, val loss 3.6074\n",
            "saving checkpoint to out\n",
            "iter 100: loss 3.5800, time 16329.98ms, mfu 2.37%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ./nanoGPT && python sample.py --dtype=float16 --num_samples=5 --max_new_tokens=10 --start=\"to be\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOjpslgu6g4Y",
        "outputId": "4b5f470f-13a0-4aa6-bf0c-3dd6bd028f7b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: dtype = float16\n",
            "Overriding: num_samples = 5\n",
            "Overriding: max_new_tokens = 10\n",
            "Overriding: start = to be\n",
            "/content/nanoGPT/sample.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(ckpt_path, map_location=device)\n",
            "number of parameters: 123.65M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "to be more virtuous than the world's\n",
            "Romeo\n",
            "---------------\n",
            "to be made so?\n",
            "\n",
            "DUKE OF YORK\n",
            "---------------\n",
            "to be,\n",
            "For my goodness is now under the\n",
            "---------------\n",
            "to be an officer, nor a good friend; but he\n",
            "---------------\n",
            "to be a pretty woman for that\n",
            "I have done here\n",
            "---------------\n"
          ]
        }
      ]
    }
  ]
}